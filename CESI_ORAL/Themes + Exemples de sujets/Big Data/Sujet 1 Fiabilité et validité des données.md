# **Plan détaillé 

## Introduction 

### Phrase d'accroche 

"Selon une étude de l'IDC, le volume de données mondiales devrait atteindre 175 Zettaoctets d'ici 2025. Imaginez un instant les ramifications si même une petite fraction de ces données était erronée ou trompeuse." (source 1)

### Contextualisation 

"Dans un monde où les données sont la monnaie de l'ère numérique, la fiabilité et la validité de ces données deviennent des enjeux cruciaux. Des décisions opérationnelles aux orientations stratégiques, de fausses données peuvent avoir un impact non seulement sur les résultats financiers, mais aussi sur la réputation et la confiance des parties prenantes."

### Problématique 

"La question qui se pose donc est la suivante : Comment s'assurer que ces vastes volumes de données sont fiables et valides? Une donnée non fiable ou invalide peut entraîner une chaîne de décisions erronées, avec des conséquences potentiellement désastreuses sur plusieurs fronts."

### Objectifs de la présentation

"Cette présentation se concentrera sur les enjeux économiques, éthiques et technologiques liés à la fiabilité et à la validité des données. Nous aborderons les mécanismes, outils et meilleures pratiques pour assurer l'intégrité des données dans le contexte d'un monde hyperconnecté et axé sur les données."

---
## 2. Présentation des enjeux 

### Enjeux Économiques 

1. **Introduction **: L'importance cruciale de la qualité des données est mise en lumière, indiquant que celle-ci a des implications économiques considérables pour les entreprises.

2. **Exemples concrets **:
    - Une entreprise de vente en ligne ayant mal intégré les données de ses fournisseurs, aboutissant à des erreurs de stocks et finalement à des pertes de revenus et des clients mécontents.
    - Une compagnie aérienne qui, à cause d'une erreur dans les données météorologiques, a rerouté plusieurs vols, engendrant des coûts supplémentaires et des indemnisations.

3. **Conclusion**: Ignorer la fiabilité des données peut avoir un impact financier dévastateur sur les entreprises.

### Enjeux Éthiques 

1. **Introduction **: Cette section est orientée vers les implications éthiques de la fiabilité et de la validité des données.

2. **Responsabilité:**
    - Un hôpital a mal géré les dossiers médicaux de ses patients, menant à des erreurs de diagnostic et de traitement.
    - Une agence gouvernementale a formulé une politique de logement sur la base de statistiques de recensement erronées, aboutissant à un malinvestissement dans les zones inappropriées.

3. **Conclusion**: L'éthique dans la gestion des données peut profondément affecter la confiance du public envers les institutions et les entreprises.

### Enjeux Technologiques 

1. **Introduction**: La complexité technique associée à la gestion de grandes quantités de données est abordée.

2. **Défis**:
    - Le cas d'une entreprise de fabrication qui a eu du mal à traiter et à nettoyer les données en temps réel provenant de ses machines industrielles.
    - L'exemple d'une ville ayant installé des capteurs IoT pour surveiller la qualité de l'air mais qui n'a pas réussi à filtrer et valider les données, causant de fausses alertes.

3. **Conclusion**: Le manquement dans la gestion technique des données a des implications économiques et éthiques significatives, ce n'est pas simplement une question d'ingénierie.

---
## 3. Analyse de l'Existant
### Concepts Théoriques

1. **Fiabilité vs Validité**
    - **Introduction**: Une brève présentation des concepts clés de la fiabilité et de la validité.
    - **Fiabilité**: Ce terme se rapporte à la cohérence des données. Un ensemble de données est considéré comme fiable s'il produit des résultats similaires dans des conditions similaires, à plusieurs reprises.
    - **Validité**: Ce terme se rapporte à la justesse ou l'exactitude des données. Un ensemble de données est valide si les données qu'il contient sont une représentation précise de la réalité.
    - **Distinction**: Alors que la fiabilité est une condition nécessaire, elle n'est pas suffisante pour la validité. Une donnée peut être fiable sans être valide.
    
2. **Data Cleaning**
    - **Introduction**: L'importance du nettoyage des données dans la gestion des enjeux liés à la fiabilité et la validité.
    - **Méthodes**: Utilisation d'algorithmes pour éliminer les valeurs aberrantes, remplir les valeurs manquantes et normaliser les données.
    - **Outils**: Présentation de quelques outils populaires tels que OpenRefine, Trifacta, ou des librairies Python comme Pandas.

### Pratiques Professionnelles et Expérience

1. **Erreurs courantes ou anomalies**
    - **Introduction**: Discussion sur les types d'erreurs couramment rencontrées.
    - **Exemples**: Données dupliquées, incohérences dans les formats, et valeurs aberrantes.
    - **Conséquences**: Impact de ces erreurs sur la prise de décision et les performances de l'entreprise.
2. **Méthodes pour assurer la fiabilité et la validité**
    - **Introduction**: L'importance de méthodes vérifiées pour maintenir la qualité des données.
    - **Techniques**: Utilisation de contrôles d'intégrité, de validation croisée, et d'algorithmes de détection d'anomalies.
    - **Suivi et Surveillance**: Mise en place de tableaux de bord pour le suivi en temps réel de la qualité des données.

### Exemples d'Autres Entreprises

1. **Erreurs de données célèbres**
    - **Introduction**: Discussion sur des cas notoires d'erreurs de données.
    - **Exemples**: Le cas de la NASA et de l'échec de la sonde Mars Climate Orbiter à cause d'une simple erreur d'unité.
2. **Best Practices**
    - **Introduction**: L'importance de suivre les meilleures pratiques dans la gestion des données.
    - **Exemples**: Google et Amazon utilisent des techniques avancées de Machine Learning pour nettoyer et vérifier leurs énormes ensembles de données.
    - **Conclusion**: L'adoption de meilleures pratiques peut servir de bouclier contre les erreurs de données coûteuses et augmenter la confiance en la qualité des données.

## 4. Propositions Stratégiques

### Outils et Techniques

1. **Introduction à des outils automatisés de nettoyage de données**
    - **Introduction**: L'importance de l'automatisation pour la gestion efficace et rentable de la qualité des données.
    - **Exemples d'outils**: Présentation d'outils tels que Talend, DataRobot et les librairies Python comme Pandas pour le nettoyage de données automatisé.
    - **Avantages**: Gain de temps, amélioration de la précision et réduction des coûts associés au nettoyage manuel des données.

2. **Méthodologies pour tester régulièrement la fiabilité et la validité**
    - **Introduction**: L'importance d'un suivi régulier pour assurer une qualité de données continue.
    - **Techniques de test**: Utilisation de méthodes statistiques, d'algorithmes de détection d'anomalies, et d'audits réguliers.
    - **Fréquence**: Importance d'établir un calendrier pour ces tests, qu'ils soient quotidiens, hebdomadaires ou mensuels en fonction de la nature des données.

### Cadres Gestionnaires et Éthiques

1. **Mise en place d'une politique de qualité des données dans l'entreprise**
    - **Introduction**: La nécessité d'avoir un cadre institutionnel pour la gestion de la qualité des données.
    - **Composants**: Définition d'objectifs, affectation de responsabilités, et création d'indicateurs clés de performance (KPI) pour évaluer la qualité des données.
    - **Mise en œuvre**: Étapes pour la mise en œuvre effective de la politique, incluant la sensibilisation des parties prenantes et la revue régulière de la politique.

2. **Formation continue des équipes sur l'importance de la qualité des données**
    - **Introduction**: Le rôle crucial de la formation dans le maintien de la qualité des données.
    - **Méthodes de Formation**: Séminaires, ateliers, et cours en ligne pour former le personnel sur les meilleures pratiques en matière de gestion des données.
    - **Suivi et Évaluation**: Mécanismes pour évaluer l'efficacité de la formation, comme des questionnaires post-formation ou des audits de qualité des données.
## 5. Conclusion

1. **Réitérer l'importance de la fiabilité et de la validité dans le contexte du Big Data**
    - **Importance du Sujet**: Souligner une dernière fois pourquoi la fiabilité et la validité des données sont d'une importance cruciale, en particulier dans l'ère du Big Data où les volumes de données sont en croissance exponentielle.
    - **Conséquences**: Résumer brièvement les risques économiques, éthiques et technologiques de la négligence de la qualité des données. Cela peut aller de la prise de décisions commerciales erronées à la perte de la confiance du public envers les institutions.

2. **Souligner la nécessité d'une approche proactive pour assurer la qualité des données**
    - **Proactivité vs Réactivité**: Mettre en évidence que la correction des erreurs de données après leur occurrence est souvent beaucoup plus coûteuse et risquée que la prévention.
    - **Appel à l'Action**: Encourager l'auditoire à adopter une approche proactive dans leur propre environnement professionnel. Suggérer de commencer par des étapes concrètes, telles que l'audit de l'état actuel de la qualité des données ou la mise en place d'un plan stratégique pour la gestion de la qualité des données.
## 6. Questions/Réponses

### Anticiper les questions possibles

1. **Comment équilibrer le coût du nettoyage des données avec les avantages potentiels ?**
    - **Préparation**: Être prêt à discuter des méthodes quantitatives et qualitatives pour évaluer le retour sur investissement (ROI) du nettoyage des données.
    - **Réponse Possible**: Expliquer que le coût initial de l'investissement dans le nettoyage des données peut être élevé, mais que les erreurs de données peuvent entraîner des coûts beaucoup plus élevés à long terme, notamment en termes de mauvaises décisions et de perte de confiance.

2. **Quels sont les défis spécifiques au Big Data par rapport à la qualité des données ?**
    - **Préparation**: Être capable de discuter des problèmes uniques posés par le volume, la variété et la vélocité du Big Data.
    - **Réponse Possible**: Mentionner que le Big Data apporte des défis supplémentaires en matière de gestion de la qualité des données, notamment en raison de la grande variété de sources de données et du volume en constante augmentation. Cela rend les techniques de nettoyage de données traditionnelles souvent inadéquates.

3. **Quels outils recommandez-vous pour la gestion de la qualité des données ?**
    - **Préparation**: Avoir une liste d'outils qui peuvent être utilisés pour différents aspects de la gestion de la qualité des données.
    - **Réponse Possible**: Suggérer des outils spécifiques pour le nettoyage des données, la validation, et le suivi de la qualité des données, en fonction des besoins spécifiques de l'auditoire.

4. **Comment intégrer une culture de la qualité des données au sein d'une organisation ?**
    - **Préparation**: Penser à des stratégies organisationnelles qui peuvent encourager une meilleure gestion des données.
    - **Réponse Possible**: Parler de l'importance de la formation et de l'engagement du leadership dans la promotion d'une culture axée sur la qualité des données.

5. **Y a-t-il des normes industrielles pour la qualité des données ?**
    - **Préparation**: Être au courant des normes ou des meilleures pratiques dans l'industrie spécifique à laquelle vous vous adressez.
    - **Réponse Possible**: Mentionner s'il existe des normes spécifiques à l'industrie, ou sinon, discuter des meilleures pratiques généralement acceptées pour la gestion de la qualité des données.

---
# Exemple + source si nécessaire (à rajouter)

(1) https://www.lebigdata.fr/big-data-2025-idc 